{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 夏目漱石の「吾輩は猫である」っぽい文を生成してみよう\n",
    "\n",
    "## 1: 分かち書き文の生成\n",
    "\n",
    "`text/wagahaiwa_nekodearu_org.txt`には、夏目漱石の小説「吾輩は猫である」の本文が記録されています。   \n",
    "このテキストを半角スペース区切りの分かち書き文に変換して、`text/wagahaiwa_nekodearu_wakati.txt`に保存してください。  \n",
    "なお、改行は削除せずにそのまま残してしておいてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "t = Tokenizer() \n",
    "\n",
    "file = 'text/wagahaiwa_nekodearu_org.txt'\n",
    "outfile = \"text/wagahaiwa_nekodearu_wakati.txt\"\n",
    "\n",
    "f = open(outfile, \"w\")\n",
    "\n",
    "with open(file) as fp:\n",
    "    line = fp.readline()\n",
    "    \n",
    "    while line:\n",
    "        token_list = t.tokenize(line, wakati=True)\n",
    "        for i in range(0, len(token_list)):\n",
    "            f.write(token_list[i])\n",
    "            if i != len(token_list) - 1:\n",
    "                f.write(' ')\n",
    "            else:\n",
    "                f.write('\\n')\n",
    "        line = fp.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吾輩 は 猫 で ある 。 名前 は まだ 無い 。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(outfile, 'r', encoding='utf-8') as fi:\n",
    "    print(fi.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ２：ランダム文生成\n",
    "\n",
    "下のセルで、課題１で作成した`text/wagahaiwa_nekodearu_wakati.txt`を入力として、統計的tri-gramモデルを学習してください。  \n",
    "学習モデルの変数名は`lm`とします。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13588\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm import Vocabulary\n",
    "from nltk.lm.models import MLE\n",
    "from nltk.util import ngrams\n",
    "\n",
    "file = \"text/wagahaiwa_nekodearu_wakati.txt\"\n",
    "words = [('<BOL> ' + l + ' <EOL>').split() for l in open(file, 'r', encoding='utf-8').readlines()]\n",
    "\n",
    "# 読み込んだ小説集の語彙（異なり単語）を収集\n",
    "# Vocabularyは1次元のリストを受け取るが、wordsは2次元のリストなので、\n",
    "# wordsを内包表記で2次元から1次元に変換してからVocabularyに渡しています\n",
    "vocab = Vocabulary([item for sublist in words for item in sublist])\n",
    "print('Vocabulary size: ' + str(len(vocab))) # 語彙サイズ（単語の種類数）\n",
    "\n",
    "text_trigrams = [ngrams(word, 3) for word in words] # tri-gramを生成\n",
    "#print('trigram',[x for x in text_trigrams[0]])\n",
    "\n",
    "n = 3\n",
    "lm = MLE(order = n, vocabulary = vocab) # 最尤推定法（Maximum Likelihood Estimation)による統計的n-gram言語モデルの準備\n",
    "lm.fit(text_trigrams) # 上で生成したtri-gramを使って言語モデルを学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吾輩は全体何とも知らないように考えるほど妙に見識張った男である。\n",
      "吾輩はやはり主人の邸内も珍らしくなくなったから、その令嬢の鼻などは詩を読んだ事ではない。\n",
      "吾輩は猫だ？長吉なんぞじゃ訳がない、ただじゃ出来ませんと主張する哲学者と云うんで、僕のうちでもっとも流行するものが出来ないのさ。\n",
      "吾輩は例のごとく深いのが悟ったようになめくじのソップの御威光となるところを主人は伯父さん心の修行をしたがその中から消え去った時先生の方を向いた。\n",
      "吾輩は今吾輩が耳を立てて弾ね返る。\n",
      "吾輩は惜気もなく我勝ちに飛び出して来て室内の動静を偵察に及んでくるんですか」\n",
      "吾輩は年来美学上の方へなりとも御承知下さい。\n",
      "吾輩は椽側へ出したり引っ込ましたり、とどのつまりが骨折り損の草臥儲けだからね」\n",
      "吾輩は四五ページには捲かれろ、強いものがあると云おうが、吾輩も一廉の水彩画を夢にもない。\n",
      "吾輩はこれに限ると考え付いた。\n"
     ]
    }
   ],
   "source": [
    "context = ['吾輩', 'は']\n",
    "\n",
    "prob_list = [(word, lm.score(word, context)) for word\n",
    "            in lm.context_counts(lm.vocab.lookup(context))]\n",
    "prob_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "sum_prob = 0.0 # 「ジョバンニは」に続く単語のtri-gram確率をすべて足すとどうなるでしょう？\n",
    "for word, prob in prob_list:\n",
    "    #print('\\t{:s}: {:f}'.format(word, prob))\n",
    "    sum_prob += prob\n",
    "\n",
    "\n",
    "### ランダム文生成 ####\n",
    "\n",
    "iterations = 10\n",
    "\n",
    "for x in range(0, iterations):\n",
    "    context = ['吾輩', 'は']\n",
    "    for i in range(0, 100):\n",
    "        # contextのうち最後の2単語から次に繋がる確率0じゃない単語をランダムに選ぶ\n",
    "        w = lm.generate(text_seed=context)\n",
    "        context.append(w) # 選ばれた単語をcontextに連結\n",
    "        #print(context)\n",
    "    \n",
    "        if '。' == w or '」' == w or 'EOL' == w: # 句点「。」か<EOP>に到達したらそこで終了\n",
    "            break\n",
    "            \n",
    "    for word in context:\n",
    "        print(word, end='')\n",
    "        \n",
    "    print('\\n', end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
